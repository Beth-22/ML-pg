# -*- coding: utf-8 -*-
"""Naive Bayes Classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RR9TZ3P_0XnBr7C16LheNTlcQAvXjwov
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from collections import defaultdict
from sklearn.naive_bayes import GaussianNB

"""# 1. Load and Prepare Data"""

print("--- 1. Data Loading and Preparation ---")
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names
target_names = iris.target_names

feature_names

target_names # species

"""# Split data into training and testing sets (80% train, 20% test)


"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Total samples: {len(X)}")
print(f"Training samples: {len(X_train)}")
print(f"Testing samples: {len(X_test)}")
print(f"Features (n): {feature_names}")
print(f"Classes (K): {target_names}")
print("\n")

"""# Data Visualization"""

def plot_data_distribution(X, y, feature_names, target_names, x_axis_idx, y_axis_idx):
    plt.figure(figsize=(10, 6))

    for i, target_name in enumerate(target_names):

        plt.scatter(
            X[y == i, x_axis_idx],
            X[y == i, y_axis_idx],
            label=target_name,
            alpha=0.7
        )

    plt.title("Iris Dataset: Feature Distribution by Class")
    plt.xlabel(feature_names[x_axis_idx])
    plt.ylabel(feature_names[y_axis_idx])
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()

print("--- 1.5 Data Visualization: Features 1 vs 2 ---")
plot_data_distribution(X, y, feature_names, target_names, 1, 2)
print("Interpretation: Naive Bayes models the mean and variance (spread) of these clusters for each class.\n")

"""# Lets build our classifier"""

class NaiveBayesClassifier:
    def fit(self, X, y):
        # Determine unique classes (K) and number of features (n)
        self.classes = np.unique(y) # find unique target classes
        self.n_features = X.shape[1] # find number of features

        # Initialize dictionaries to store mean, variance, and prior probabilities
        self.class_mean = {}
        self.class_var = {}
        self.priors = {}

        for c in self.classes:
            X_c = X[y == c] # all data to specific class

            # 2a. Calculate Prior P(C_k)
            # Prior = (Number of samples in class c) / (Total samples)
            self.priors[c] = len(X_c) / len(X)

            # 2b. Calculate Mean and Variance for each feature (for Likelihood)
            self.class_mean[c] = X_c.mean(axis=0)
            self.class_var[c] = X_c.var(axis=0)

    # Helper function: Gaussian Probability Density Function (PDF)
    def _gaussian_pdf(self, x, mean, var):
        # P(x_i | C_k) calculation based on Gaussian distribution
        epsilon = 1e-6  # Small value to prevent division by zero in variance
        numerator = np.exp(-((x - mean) ** 2) / (2 * (var + epsilon)))
        denominator = np.sqrt(2 * np.pi * (var + epsilon))
        return numerator / denominator

    def predict(self, X):e
        predictions = [self._predict(x) for x in X]
        return np.array(predictions)

    def _predict(self, x):
        # Dictionary to hold the final score for each class
        posterior_scores = {}

        for c in self.classes:
            prior = self.priors[c]
            likelihood = 1.0

            # Naive Likelihood Calculation (Product Rule)
            for i in range(self.n_features):
                # Calculate P(x_i | C_k)
                feature_likelihood = self._gaussian_pdf(
                    x[i],
                    self.class_mean[c][i],
                    self.class_var[c][i]
                )
                # Apply Naive Independence Assumption: P(X|y) = P(x1|y) * P(x2|y) * ...
                likelihood *= feature_likelihood

            # Posterior Score = Likelihood * Prior
            posterior_score = likelihood * prior
            posterior_scores[c] = posterior_score

        # Argmax Decision: Pick the class (k) that yields the highest score.
        predicted_class = max(posterior_scores, key=posterior_scores.get)

        return predicted_class

"""# **Training and Demonstration**"""

nb = NaiveBayesClassifier()
nb.fit(X_train, y_train)

"""# Training Results (Prior and Feature Means/Variances)"""

print("--- 2. Training Results (Prior and Feature Means/Variances) ---")
for c in nb.classes:
    class_name = target_names[c]
    print(f"Class: {class_name} ({c})")
    print(f"  Prior P({class_name}): {nb.priors[c]:.4f}")
    # Show mean and variance for all features
    for i in range(nb.n_features):
        print(f"    - '{feature_names[i]}': Mean={nb.class_mean[c][i]:.4f}, Var={nb.class_var[c][i]:.4f}")
    print("-" * 20)

test_sample_index = 0
x_test_single = X_test[test_sample_index]
true_class_label = target_names[y_test[test_sample_index]]

print("\n--- 3. Detailed Trace for a Single Sample (The Argmax Input) ---")
print(f"Test Features (X): {x_test_single}")
print(f"True Class: {true_class_label}\n")

print("--- 6. Normal Distribution Visualization for Each Feature and Class (with Test Sample Highlight) ---")

plt.figure(figsize=(15, 10))

# Define a range for x-axis for plotting PDFs, based on min/max of each feature
feature_ranges = []
for i in range(nb.n_features):
    min_val = np.min(X[:, i])
    max_val = np.max(X[:, i])
    feature_ranges.append(np.linspace(min_val - 0.5, max_val + 0.5, 1000)) # this will create evenly spaced thing for the chart we are going to build

# Get default color cycle for consistency
prop_cycle = plt.rcParams['axes.prop_cycle']
colors = prop_cycle.by_key()['color']

nb.class_mean

x_test_single

plt.figure(figsize=(15, 10))

for i in range(nb.n_features):
    plt.subplot(2, 2, i + 1) # Arrange plots in a 2x2 grid rows, columns then index
    x_values = feature_ranges[i]

    # Store PDF values for test sample for each class to plot markers later
    test_sample_pdf_values = {}

    for c_idx, c in enumerate(nb.classes):
        class_name = target_names[c]
        mean = nb.class_mean[c][i] # mean of some class and some feature/column
        var = nb.class_var[c][i] # var of some class and some feature/column

        # Calculate PDF values for the curve
        pdf_values = [nb._gaussian_pdf(x_val, mean, var) for x_val in x_values]

        plt.plot(x_values, pdf_values, label=f'{class_name}', color=colors[c_idx])

        # Calculate PDF value for the specific test sample point
        test_sample_pdf_value = nb._gaussian_pdf(x_test_single[i], mean, var)
        test_sample_pdf_values[c] = test_sample_pdf_value

        # Plot a marker for the test sample on this class's curve
        plt.scatter(x_test_single[i], test_sample_pdf_value, color=colors[c_idx], s=100, marker='X', zorder=5)


    # Add a single vertical line for the selected test sample's feature value
    # Only add to legend once
    if i == 0:
      plt.axvline(x=x_test_single[i], color='red', linestyle='--', label='Test Sample Value')
    else:
      plt.axvline(x=x_test_single[i], color='red', linestyle='--')

    plt.title(f'Feature: {feature_names[i]}')
    plt.xlabel('Value')
    plt.ylabel('Probability Density')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

print("Interpretation: Each plot shows the Gaussian distribution for one feature, separated by class. The Naive Bayes classifier uses these distributions to determine the likelihood of a new data point belonging to each class.")
print("The red dashed line indicates the value of the selected test sample for that specific feature. The 'X' markers on each class's curve show the probability density (likelihood) of that sample's feature value for each respective class.")

"""# Manually trace the prediction process for the single sample"""

predicted_class_index = nb.predict(np.array([x_test_single]))[0]
predicted_class_label = target_names[predicted_class_index]

print("\n--- 3. Prediction for a Single Sample using nb.predict() ---")
print(f"Test Features (X): {x_test_single}")
print(f"True Class: {true_class_label}\n")
print(f"Predicted Class (using nb.predict()): {predicted_class_label} (Index: {predicted_class_index})")

# --- 5. Full Model Evaluation ---
y_pred = nb.predict(X_test)

print("\n--- 5. Model Evaluation Metrics ---")

# 5a. Classification Report
accuracy = np.sum(y_pred == y_test) / len(y_test)
print(f"Overall Accuracy on Test Set: {accuracy * 100:.2f}%\n")

print("--- Classification Report (Precision, Recall, F1-Score) ---")
print(classification_report(y_test, y_pred, target_names=target_names))

# 5b. Confusion Matrix Visualization
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)

print("--- Confusion Matrix ---")
# Plotting the confusion matrix for visual interpretation
fig, ax = plt.subplots(figsize=(8, 8))
disp.plot(ax=ax, cmap=plt.cm.Blues)
ax.set_title("Confusion Matrix for Naive Bayes Classifier")
plt.show()

print("\nInterpretation: The confusion matrix shows how many samples were correctly (diagonal) and incorrectly (off-diagonal) classified.")

"""# Is there a simple way to do those predictions, instead of doing all those math?"""

print("--- Scikit-learn GaussianNB Predictions on Test Set ---")

# Instantiate and train scikit-learn's Gaussian Naive Bayes
sk_gnb = GaussianNB()
sk_gnb.fit(X_train, y_train)

# Make predictions on the test set
sk_y_pred = sk_gnb.predict(X_test)

print("\n--- Scikit-learn GaussianNB Model Evaluation Metrics ---")

# Calculate and print overall accuracy
sk_accuracy = np.sum(sk_y_pred == y_test) / len(y_test)
print(f"Overall Accuracy on Test Set (Scikit-learn GNB): {sk_accuracy * 100:.2f}%\n")

# Print classification report
print("--- Scikit-learn GNB Classification Report (Precision, Recall, F1-Score) ---")
print(classification_report(y_test, sk_y_pred, target_names=target_names))

# Generate and display confusion matrix
sk_cm = confusion_matrix(y_test, sk_y_pred)
sk_disp = ConfusionMatrixDisplay(confusion_matrix=sk_cm, display_labels=target_names)

print("\n--- Scikit-learn GNB Confusion Matrix ---")
fig, ax = plt.subplots(figsize=(8, 8))
sk_disp.plot(ax=ax, cmap=plt.cm.Greens)
ax.set_title("Confusion Matrix for Scikit-learn GaussianNB Classifier")
plt.show()

print("\nInterpretation: This shows the performance of the scikit-learn Gaussian Naive Bayes classifier on the test set, including its accuracy, precision, recall, f1-score, and a visual confusion matrix.")